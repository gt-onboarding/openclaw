---
title: Comprensi√≥n de medios
summary: "Comprensi√≥n de im√°genes/audio/video entrantes (opcional) con proveedor + CLI como respaldo"
read_when:
  - Dise√±ando o refactorizando la comprensi√≥n de medios
  - Ajustando el preprocesamiento de contenido de audio/video/imagen entrante
---

<div id="media-understanding-inbound-2026-01-17">
  # Comprensi√≥n de medios (entrante) ‚Äî 2026-01-17
</div>

OpenClaw puede **resumir medios entrantes** (imagen/audio/video) antes de que se ejecute el pipeline de respuesta. Detecta autom√°ticamente cu√°ndo hay herramientas locales o claves de proveedor disponibles, y puede desactivarse o personalizarse. Si la comprensi√≥n est√° desactivada, los modelos siguen recibiendo los archivos/URL originales como siempre.

<div id="goals">
  ## Objetivos
</div>

* Opcional: preprocesar el contenido multimedia entrante en texto breve para un enrutamiento m√°s r√°pido y un mejor an√°lisis de comandos.
* Conservar siempre el contenido multimedia original al entregarlo al modelo.
* Admitir **APIs de proveedores** y mecanismos de reserva v√≠a **CLI**.
* Permitir m√∫ltiples modelos con mecanismos de reserva en orden de prioridad (error/tama√±o/timeout).

<div id="highlevel-behavior">
  ## Comportamiento de alto nivel
</div>

1. Recopila los archivos adjuntos entrantes (`MediaPaths`, `MediaUrls`, `MediaTypes`).
2. Para cada capacidad habilitada (imagen/audio/video), selecciona archivos adjuntos seg√∫n la pol√≠tica (predeterminado: **primero**).
3. Elige la primera entrada de modelo elegible (tama√±o + capacidad + autenticaci√≥n).
4. Si un modelo falla o el contenido multimedia es demasiado grande, **pasa a la siguiente entrada**.
5. En caso de √©xito:
   * `Body` se convierte en un bloque `[Image]`, `[Audio]` o `[Video]`.
   * El audio establece `{{Transcript}}`; el an√°lisis de comandos usa el texto de los subt√≠tulos cuando est√° presente,
     en caso contrario usa la transcripci√≥n.
   * Los subt√≠tulos se conservan como `User text:` dentro del bloque.

Si la comprensi√≥n falla o est√° deshabilitada, **el flujo de respuesta contin√∫a** con el cuerpo original + archivos adjuntos.

<div id="config-overview">
  ## Descripci√≥n general de la configuraci√≥n
</div>

`tools.media` admite **modelos compartidos** m√°s anulaciones por capacidad:

* `tools.media.models`: lista de modelos compartidos (usa `capabilities` para limitar).
* `tools.media.image` / `tools.media.audio` / `tools.media.video`:
  * valores predeterminados (`prompt`, `maxChars`, `maxBytes`, `timeoutSeconds`, `language`)
  * anulaciones de proveedor (`baseUrl`, `headers`, `providerOptions`)
  * opciones de audio de Deepgram mediante `tools.media.audio.providerOptions.deepgram`
  * **lista opcional de `models` por capacidad** (preferida frente a los modelos compartidos)
  * pol√≠tica de `attachments` (`mode`, `maxAttachments`, `prefer`)
  * `scope` (limitaci√≥n opcional por clave de canal/chatType/session)
* `tools.media.concurrency`: m√°ximo de ejecuciones concurrentes de capacidades (valor predeterminado **2**).

```json5
{
  tools: {
    media: {
      models: [ /* shared list */ ],
      image: { /* sobrescrituras opcionales */ },
      audio: { /* sobrescrituras opcionales */ },
      video: { /* sobrescrituras opcionales */ }
    }
  }
}
```

<div id="model-entries">
  ### Entradas de modelo
</div>

Cada entrada de `models[]` puede ser de tipo **proveedor** o **CLI**:

```json5
{
  type: "provider",        // default if omitted
  provider: "openai",
  model: "gpt-5.2",
  prompt: "Describe the image in <= 500 chars.",
  maxChars: 500,
  maxBytes: 10485760,
  timeoutSeconds: 60,
  capabilities: ["image"], // opcional, se usa para entradas multimodales
  profile: "vision-profile",
  preferredProfile: "vision-fallback"
}
```

```json5
{
  type: "cli",
  command: "gemini",
  args: [
    "-m",
    "gemini-3-flash",
    "--allowed-tools",
    "read_file",
    "Read the media at {{MediaPath}} and describe it in <= {{MaxChars}} characters."
  ],
  maxChars: 500,
  maxBytes: 52428800,
  timeoutSeconds: 120,
  capabilities: ["video", "image"]
}
```

Las plantillas de la CLI tambi√©n pueden utilizar:

* `{{MediaDir}}` (directorio que contiene el archivo multimedia)
* `{{OutputDir}}` (directorio de trabajo temporal creado para esta ejecuci√≥n)
* `{{OutputBase}}` (ruta base del archivo temporal, sin extensi√≥n)

<div id="defaults-and-limits">
  ## Valores predeterminados y l√≠mites
</div>

Valores predeterminados recomendados:

* `maxChars`: **500** para imagen/v√≠deo (corto, apto para comandos)
* `maxChars`: **unset** para audio (transcripci√≥n completa, salvo que definas un l√≠mite)
* `maxBytes`:
  * imagen: **10MB**
  * audio: **20MB**
  * v√≠deo: **50MB**

Reglas:

* Si el archivo multimedia supera `maxBytes`, ese modelo se omite y se **prueba el siguiente modelo**.
* Si el modelo devuelve m√°s de `maxChars`, la salida se trunca.
* `prompt` usa por defecto el sencillo ‚ÄúDescribe the {media}.‚Äù m√°s la indicaci√≥n de `maxChars` (solo imagen/v√≠deo).
* Si `<capability>.enabled: true` pero no hay modelos configurados, OpenClaw intenta usar el
  **modelo de respuesta activo** cuando su proveedor admite esta capacidad.

<div id="auto-detect-media-understanding-default">
  ### Detecci√≥n autom√°tica de comprensi√≥n de contenido multimedia (predeterminado)
</div>

Si `tools.media.<capability>.enabled` **no** est√° establecido en `false` y no has
configurado modelos, OpenClaw detecta autom√°ticamente en este orden y **se detiene en la primera
opci√≥n que funciona**:

1. **CLIs locales** (solo audio; si est√°n instaladas)
   * `sherpa-onnx-offline` (requiere `SHERPA_ONNX_MODEL_DIR` con encoder/decoder/joiner/tokens)
   * `whisper-cli` (`whisper-cpp`; usa `WHISPER_CPP_MODEL` o el modelo tiny incluido)
   * `whisper` (CLI de Python; descarga modelos autom√°ticamente)
2. **Gemini CLI** (`gemini`) usando `read_many_files`
3. **Claves de proveedor**
   * Audio: OpenAI ‚Üí Groq ‚Üí Deepgram ‚Üí Google
   * Imagen: OpenAI ‚Üí Anthropic ‚Üí Google ‚Üí MiniMax
   * Video: Google

Para desactivar la detecci√≥n autom√°tica, establece:

```json5
{
  tools: {
    media: {
      audio: {
        enabled: false
      }
    }
  }
}
```

Nota: la detecci√≥n del binario es de tipo best-effort en macOS/Linux/Windows; aseg√∫rate de que la CLI est√© en `PATH` (expandimos `~`) o establece un modelo CLI expl√≠cito con la ruta completa del comando.

<div id="capabilities-optional">
  ## Capacidades (opcional)
</div>

Si configuras `capabilities`, la entrada solo se ejecuta para esos tipos de medios. Para listas compartidas, OpenClaw puede inferir valores predeterminados:

* `openai`, `anthropic`, `minimax`: **imagen**
* `google` (Gemini API): **imagen + audio + video**
* `groq`: **audio**
* `deepgram`: **audio**

Para entradas de CLI, **configura `capabilities` expl√≠citamente** para evitar coincidencias inesperadas.
Si omites `capabilities`, la entrada podr√° usarse en la lista en la que aparece.

<div id="provider-support-matrix-openclaw-integrations">
  ## Matriz de soporte de proveedores (integraciones de OpenClaw)
</div>

| Capacidad | Integraci√≥n de proveedor | Notas |
|----------|--------------------------|-------|
| Imagen | OpenAI / Anthropic / Google / otros mediante `pi-ai` | Cualquier modelo del registro con capacidad de imagen funciona. |
| Audio | OpenAI, Groq, Deepgram, Google | Transcripci√≥n del proveedor (Whisper/Deepgram/Gemini). |
| Video | Google (Gemini API) | Comprensi√≥n de video del proveedor. |

<div id="recommended-providers">
  ## Proveedores recomendados
</div>

**Imagen**

* Usa tu modelo activo si admite im√°genes.
* Buenos valores predeterminados: `openai/gpt-5.2`, `anthropic/claude-opus-4-5`, `google/gemini-3-pro-preview`.

**Audio**

* `openai/gpt-4o-mini-transcribe`, `groq/whisper-large-v3-turbo` o `deepgram/nova-3`.
* Fallback en la CLI: `whisper-cli` (whisper-cpp) o `whisper`.
* Configuraci√≥n de Deepgram: [Deepgram (transcripci√≥n de audio)](/es/providers/deepgram).

**Video**

* `google/gemini-3-flash-preview` (r√°pido), `google/gemini-3-pro-preview` (m√°s completo).
* Fallback en la CLI: CLI `gemini` (admite `read_file` en v√≠deo/audio).

<div id="attachment-policy">
  ## Pol√≠tica de adjuntos
</div>

Para cada capacidad, `attachments` controla qu√© adjuntos se procesan:

* `mode`: `first` (predeterminado) o `all`
* `maxAttachments`: limita el n√∫mero de adjuntos procesados (valor predeterminado **1**)
* `prefer`: `first`, `last`, `path`, `url`

Cuando `mode: "all"`, los resultados se etiquetan como `[Image 1/2]`, `[Audio 2/2]`, etc.

<div id="config-examples">
  ## Ejemplos de configuraci√≥n
</div>

<div id="1-shared-models-list-overrides">
  ### 1) Lista de modelos compartidos + sobrescrituras
</div>

```json5
{
  tools: {
    media: {
      models: [
        { provider: "openai", model: "gpt-5.2", capabilities: ["image"] },
        { provider: "google", model: "gemini-3-flash-preview", capabilities: ["image", "audio", "video"] },
        {
          type: "cli",
          command: "gemini",
          args: [
            "-m",
            "gemini-3-flash",
            "--allowed-tools",
            "read_file",
            "Lee el medio en {{MediaPath}} y descr√≠belo en <= {{MaxChars}} caracteres."
          ],
          capabilities: ["image", "video"]
        }
      ],
      audio: {
        attachments: { mode: "all", maxAttachments: 2 }
      },
      video: {
        maxChars: 500
      }
    }
  }
}
```

<div id="2-audio-video-only-image-off">
  ### 2) Solo audio y video (sin imagen)
</div>

```json5
{
  tools: {
    media: {
      audio: {
        enabled: true,
        models: [
          { provider: "openai", model: "gpt-4o-mini-transcribe" },
          {
            type: "cli",
            command: "whisper",
            args: ["--model", "base", "{{MediaPath}}"]
          }
        ]
      },
      video: {
        enabled: true,
        maxChars: 500,
        models: [
          { provider: "google", model: "gemini-3-flash-preview" },
          {
            type: "cli",
            command: "gemini",
            args: [
              "-m",
              "gemini-3-flash",
              "--allowed-tools",
              "read_file",
              "Read the media at {{MediaPath}} and describe it in <= {{MaxChars}} characters."
            ]
          }
        ]
      }
    }
  }
}
```

<div id="3-optional-image-understanding">
  ### 3) Comprensi√≥n opcional de im√°genes
</div>

```json5
{
  tools: {
    media: {
      image: {
        enabled: true,
        maxBytes: 10485760,
        maxChars: 500,
        models: [
          { provider: "openai", model: "gpt-5.2" },
          { provider: "anthropic", model: "claude-opus-4-5" },
          {
            type: "cli",
            command: "gemini",
            args: [
              "-m",
              "gemini-3-flash",
              "--allowed-tools",
              "read_file",
              "Read the media at {{MediaPath}} and describe it in <= {{MaxChars}} characters."
            ]
          }
        ]
      }
    }
  }
}
```

<div id="4-multimodal-single-entry-explicit-capabilities">
  ### 4) Entrada √∫nica multimodal (capacidades expl√≠citas)
</div>

```json5
{
  tools: {
    media: {
      image: { models: [{ provider: "google", model: "gemini-3-pro-preview", capabilities: ["image", "video", "audio"] }] },
      audio: { models: [{ provider: "google", model: "gemini-3-pro-preview", capabilities: ["image", "video", "audio"] }] },
      video: { models: [{ provider: "google", model: "gemini-3-pro-preview", capabilities: ["image", "video", "audio"] }] }
    }
  }
}
```

<div id="status-output">
  ## Salida de estado
</div>

Cuando se ejecuta Media Understanding, `/status` incluye una breve l√≠nea de resumen:

```
üìé Media: image ok (openai/gpt-5.2) ¬∑ audio skipped (maxBytes)
```

Esto muestra los resultados por capacidad y, cuando corresponda, el proveedor/modelo seleccionado.

<div id="notes">
  ## Notas
</div>

* La comprensi√≥n es de tipo **best‚Äëeffort**: los errores no bloquean las respuestas.
* Los archivos adjuntos se siguen enviando a los modelos incluso cuando la comprensi√≥n est√° deshabilitada.
* Usa `scope` para limitar d√≥nde se ejecuta la comprensi√≥n (por ejemplo, solo en mensajes directos).

<div id="related-docs">
  ## Documentos relacionados
</div>

* [Configuraci√≥n](/es/gateway/configuration)
* [Soporte de im√°genes y contenido multimedia](/es/nodes/images)