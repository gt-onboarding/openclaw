---
title: Medienverst√§ndnis
summary: "Verstehen eingehender Bild-/Audio-/Video-Daten (optional) mit Anbieter- und CLI-Fallbacks"
read_when:
  - Entwurf oder √úberarbeitung des Medienverst√§ndnisses
  - Feinabstimmung der Vorverarbeitung eingehender Audio-/Video-/Bilddaten
---

<div id="media-understanding-inbound-2026-01-17">
  # Medienverst√§ndnis (eingehend) ‚Äî 2026-01-17
</div>

OpenClaw kann **eingehende Medien** (Bild/Audio/Video) zusammenfassen, bevor die Antwort-Pipeline ausgef√ºhrt wird. Es erkennt automatisch, ob lokale Tools oder Anbieterschl√ºssel verf√ºgbar sind, und kann deaktiviert oder angepasst werden. Wenn das Medienverst√§ndnis deaktiviert ist, erhalten Modelle die urspr√ºnglichen Dateien/URLs trotzdem wie gewohnt.

<div id="goals">
  ## Ziele
</div>

* Optional: eingehende Medien vorverarbeiten und zu kurzem Text verdichten, um schnelleres Routing und bessere Befehlsinterpretation zu erm√∂glichen.
* Originalmedien bei der √úbergabe an das Modell stets unver√§ndert beibehalten.
* **Anbieter-APIs** und **CLI-Fallbacks** unterst√ºtzen.
* Mehrere Modelle mit geordneter Fallback-Reihenfolge (Fehler/Gr√∂√üe/Timeout) erm√∂glichen.

<div id="highlevel-behavior">
  ## Verhalten auf hoher Ebene
</div>

1. Eingehende Anh√§nge erfassen (`MediaPaths`, `MediaUrls`, `MediaTypes`).
2. F√ºr jede aktivierte F√§higkeit (Bild/Audio/Video) Anh√§nge gem√§√ü Richtlinie ausw√§hlen (Standard: **erster**).
3. Den ersten geeigneten Modelleintrag ausw√§hlen (Gr√∂√üe + F√§higkeit + Auth).
4. Wenn ein Modell fehlschl√§gt oder das Medium zu gro√ü ist, **auf den n√§chsten Eintrag zur√ºckgreifen**.
5. Bei Erfolg:
   * `Body` wird zu einem `[Image]`‚Äë, `[Audio]`‚Äë oder `[Video]`‚ÄëBlock.
   * Audio setzt `{{Transcript}}`; das Befehls‚ÄëParsing verwendet den Untertiteltext, wenn vorhanden,
     andernfalls das Transkript.
   * Untertitel werden als `User text:` innerhalb des Blocks beibehalten.

Wenn das Verst√§ndnis fehlschl√§gt oder deaktiviert ist, **l√§uft der Antwort‚ÄëFlow mit dem urspr√ºnglichen Body + Anh√§ngen weiter**.

<div id="config-overview">
  ## Konfigurations√ºbersicht
</div>

`tools.media` unterst√ºtzt **gemeinsame Modelle** plus √úberschreibungen pro F√§higkeit:

* `tools.media.models`: gemeinsame Modellliste (verwenden Sie `capabilities` f√ºr das Gating).
* `tools.media.image` / `tools.media.audio` / `tools.media.video`:
  * Vorgaben (`prompt`, `maxChars`, `maxBytes`, `timeoutSeconds`, `language`)
  * Anbieter‚Äë√úberschreibungen (`baseUrl`, `headers`, `providerOptions`)
  * Deepgram‚ÄëAudiooptionen √ºber `tools.media.audio.providerOptions.deepgram`
  * optionale **`models`‚ÄëListe pro F√§higkeit** (bevorzugt vor gemeinsamen Modellen)
  * `attachments`‚ÄëRichtlinie (`mode`, `maxAttachments`, `prefer`)
  * `scope` (optionales Gating nach channel/chatType/session‚ÄëKey)
* `tools.media.concurrency`: maximale Anzahl gleichzeitiger Capability‚ÄëAusf√ºhrungen (Standard **2**).

```json5
{
  tools: {
    media: {
      models: [ /* shared list */ ],
      image: { /* optionale √úberschreibungen */ },
      audio: { /* optionale √úberschreibungen */ },
      video: { /* optionale √úberschreibungen */ }
    }
  }
}
```

<div id="model-entries">
  ### Modelleintr√§ge
</div>

Jeder `models[]`-Eintrag kann entweder ein **anbieter** oder die **CLI** sein:

```json5
{
  type: "provider",        // default if omitted
  provider: "openai",
  model: "gpt-5.2",
  prompt: "Describe the image in <= 500 chars.",
  maxChars: 500,
  maxBytes: 10485760,
  timeoutSeconds: 60,
  capabilities: ["image"], // optional, f√ºr multimodale Eintr√§ge verwendet
  profile: "vision-profile",
  preferredProfile: "vision-fallback"
}
```

```json5
{
  type: "cli",
  command: "gemini",
  args: [
    "-m",
    "gemini-3-flash",
    "--allowed-tools",
    "read_file",
    "Read the media at {{MediaPath}} and describe it in <= {{MaxChars}} characters."
  ],
  maxChars: 500,
  maxBytes: 52428800,
  timeoutSeconds: 120,
  capabilities: ["video", "image"]
}
```

CLI-Vorlagen k√∂nnen au√üerdem Folgendes verwenden:

* `{{MediaDir}}` (Verzeichnis, das die Mediendatei enth√§lt)
* `{{OutputDir}}` (tempor√§res Arbeitsverzeichnis, das f√ºr diesen Lauf erstellt wird)
* `{{OutputBase}}` (Basis-Pfad der tempor√§ren Datei, ohne Dateiendung)

<div id="defaults-and-limits">
  ## Standardwerte und Limits
</div>

Empfohlene Standardwerte:

* `maxChars`: **500** f√ºr Bild/Video (kurz, CLI‚Äë/befehlsfreundlich)
* `maxChars`: **nicht gesetzt** f√ºr Audio (vollst√§ndiges Transkript, sofern du kein Limit setzt)
* `maxBytes`:
  * Bild: **10MB**
  * Audio: **20MB**
  * Video: **50MB**

Regeln:

* Wenn der Medieninhalt `maxBytes` √ºberschreitet, wird dieses Modell √ºbersprungen und das **n√§chste Modell ausprobiert**.
* Wenn das Modell mehr als `maxChars` zur√ºckgibt, wird die Ausgabe gek√ºrzt.
* `prompt` ist standardm√§√üig ein einfacher Text ‚ÄûDescribe the {media}.‚Äú plus die `maxChars`‚ÄëVorgabe (nur Bild/Video).
* Wenn `<capability>.enabled: true` gesetzt ist, aber keine Modelle konfiguriert sind, versucht OpenClaw das
  **aktive Antwortmodell**, sofern dessen Anbieter diese Funktion unterst√ºtzt.

<div id="auto-detect-media-understanding-default">
  ### Medienverst√§ndnis automatisch erkennen (Standard)
</div>

Wenn `tools.media.<capability>.enabled` **nicht** auf `false` gesetzt ist und du
keine Modelle konfiguriert hast, f√ºhrt OpenClaw in dieser Reihenfolge eine automatische Erkennung durch
und **stoppt bei der ersten funktionierenden Option**:

1. **Lokale CLIs** (nur Audio; falls installiert)
   * `sherpa-onnx-offline` (erfordert `SHERPA_ONNX_MODEL_DIR` mit Encoder/Decoder/Joiner/Tokens)
   * `whisper-cli` (`whisper-cpp`; verwendet `WHISPER_CPP_MODEL` oder das mitgelieferte tiny-Modell)
   * `whisper` (Python-CLI; l√§dt Modelle automatisch herunter)
2. **Gemini CLI** (`gemini`) mit `read_many_files`
3. **Anbieterschl√ºssel**
   * Audio: OpenAI ‚Üí Groq ‚Üí Deepgram ‚Üí Google
   * Bild: OpenAI ‚Üí Anthropic ‚Üí Google ‚Üí MiniMax
   * Video: Google

Um die automatische Erkennung zu deaktivieren, setze:

```json5
{
  tools: {
    media: {
      audio: {
        enabled: false
      }
    }
  }
}
```

Hinweis: Die Erkennung von Binaries erfolgt auf macOS/Linux/Windows bestm√∂glich; stelle sicher, dass die CLI im `PATH` liegt (wir expandieren `~`), oder setze ein explizites CLI-Modell mit einem vollst√§ndigen Befehlspfad.

<div id="capabilities-optional">
  ## F√§higkeiten (optional)
</div>

Wenn du `capabilities` setzt, gilt der Eintrag nur f√ºr diese Medientypen. F√ºr gemeinsame
Listen kann OpenClaw Standardwerte ableiten:

* `openai`, `anthropic`, `minimax`: **image**
* `google` (Gemini API): **image + audio + video**
* `groq`: **audio**
* `deepgram`: **audio**

F√ºr CLI-Eintr√§ge **setze `capabilities` explizit**, um unerwartete Treffer zu vermeiden.
Wenn du `capabilities` wegl√§sst, kann der Eintrag in der Liste, in der er erscheint, verwendet werden.

<div id="provider-support-matrix-openclaw-integrations">
  ## Support-Matrix f√ºr Anbieter (OpenClaw-Integrationen)
</div>

| F√§higkeit | Anbieter-Integration | Anmerkungen |
|----------|----------------------|-------------|
| Bild     | OpenAI / Anthropic / Google / andere √ºber `pi-ai` | Jedes Modell mit Bildunterst√ºtzung in der Registry funktioniert. |
| Audio    | OpenAI, Groq, Deepgram, Google | Anbieter-Transkription (Whisper/Deepgram/Gemini). |
| Video    | Google (Gemini API) | Anbieter-Videoverst√§ndnis. |

<div id="recommended-providers">
  ## Empfohlene Anbieter
</div>

**Bild**

* Verwende bevorzugt dein aktives Modell, wenn es Bilder unterst√ºtzt.
* Gute Standardvorgaben: `openai/gpt-5.2`, `anthropic/claude-opus-4-5`, `google/gemini-3-pro-preview`.

**Audio**

* `openai/gpt-4o-mini-transcribe`, `groq/whisper-large-v3-turbo` oder `deepgram/nova-3`.
* CLI-Fallback: `whisper-cli` (whisper-cpp) oder `whisper`.
* Deepgram-Konfiguration: [Deepgram (Audio-Transkription)](/de/providers/deepgram).

**Video**

* `google/gemini-3-flash-preview` (schnell), `google/gemini-3-pro-preview` (umfangreicher).
* CLI-Fallback: `gemini` CLI (unterst√ºtzt `read_file` f√ºr Video und Audio).

<div id="attachment-policy">
  ## Richtlinie f√ºr Anh√§nge
</div>

Die je F√§higkeit konfigurierbare Einstellung `attachments` steuert, welche Anh√§nge verarbeitet werden:

* `mode`: `first` (Standard) oder `all`
* `maxAttachments`: begrenzt die Anzahl der verarbeiteten Anh√§nge (Standardwert **1**)
* `prefer`: `first`, `last`, `path`, `url`

Wenn `mode: "all"` verwendet wird, werden Ausgaben als `[Image 1/2]`, `[Audio 2/2]` usw. gekennzeichnet.

<div id="config-examples">
  ## Konfigurationsbeispiele
</div>

<div id="1-shared-models-list-overrides">
  ### 1) Gemeinsame Modellliste + √úberschreibungen
</div>

```json5
{
  tools: {
    media: {
      models: [
        { provider: "openai", model: "gpt-5.2", capabilities: ["image"] },
        { provider: "google", model: "gemini-3-flash-preview", capabilities: ["image", "audio", "video"] },
        {
          type: "cli",
          command: "gemini",
          args: [
            "-m",
            "gemini-3-flash",
            "--allowed-tools",
            "read_file",
            "Read the media at {{MediaPath}} and describe it in <= {{MaxChars}} characters."
          ],
          capabilities: ["image", "video"]
        }
      ],
      audio: {
        attachments: { mode: "all", maxAttachments: 2 }
      },
      video: {
        maxChars: 500
      }
    }
  }
}
```

<div id="2-audio-video-only-image-off">
  ### 2) Nur Audio + Video (ohne Bild)
</div>

```json5
{
  tools: {
    media: {
      audio: {
        enabled: true,
        models: [
          { provider: "openai", model: "gpt-4o-mini-transcribe" },
          {
            type: "cli",
            command: "whisper",
            args: ["--model", "base", "{{MediaPath}}"]
          }
        ]
      },
      video: {
        enabled: true,
        maxChars: 500,
        models: [
          { provider: "google", model: "gemini-3-flash-preview" },
          {
            type: "cli",
            command: "gemini",
            args: [
              "-m",
              "gemini-3-flash",
              "--allowed-tools",
              "read_file",
              "Read the media at {{MediaPath}} and describe it in <= {{MaxChars}} characters."
            ]
          }
        ]
      }
    }
  }
}
```

<div id="3-optional-image-understanding">
  ### 3) Optionales Bildverst√§ndnis
</div>

```json5
{
  tools: {
    media: {
      image: {
        enabled: true,
        maxBytes: 10485760,
        maxChars: 500,
        models: [
          { provider: "openai", model: "gpt-5.2" },
          { provider: "anthropic", model: "claude-opus-4-5" },
          {
            type: "cli",
            command: "gemini",
            args: [
              "-m",
              "gemini-3-flash",
              "--allowed-tools",
              "read_file",
              "Lesen Sie die Medien unter {{MediaPath}} und beschreiben Sie sie in <= {{MaxChars}} Zeichen."
            ]
          }
        ]
      }
    }
  }
}
```

<div id="4-multimodal-single-entry-explicit-capabilities">
  ### 4) Multimodaler Single-Entry-Punkt (explizite F√§higkeiten)
</div>

```json5
{
  tools: {
    media: {
      image: { models: [{ provider: "google", model: "gemini-3-pro-preview", capabilities: ["image", "video", "audio"] }] },
      audio: { models: [{ provider: "google", model: "gemini-3-pro-preview", capabilities: ["image", "video", "audio"] }] },
      video: { models: [{ provider: "google", model: "gemini-3-pro-preview", capabilities: ["image", "video", "audio"] }] }
    }
  }
}
```

<div id="status-output">
  ## Statusausgabe
</div>

Wenn Media Understanding ausgef√ºhrt wird, enth√§lt `/status` eine kurze Zusammenfassungszeile:

```
üìé Media: image ok (openai/gpt-5.2) ¬∑ audio skipped (maxBytes)
```

Dies zeigt die Ergebnisse je F√§higkeit sowie den gew√§hlten Anbieter bzw. das gew√§hlte Modell, sofern zutreffend.

<div id="notes">
  ## Hinweise
</div>

* Understanding arbeitet nach dem **Best‚ÄëEffort**‚ÄëPrinzip. Fehler verhindern Antworten nicht.
* Anh√§nge werden weiterhin an Modelle √ºbergeben, selbst wenn Understanding deaktiviert ist.
* Verwende `scope`, um einzuschr√§nken, wo Understanding ausgef√ºhrt wird (z.‚ÄØB. nur DMs).

<div id="related-docs">
  ## Verwandte Dokumente
</div>

* [Konfiguration](/de/gateway/configuration)
* [Bild- und Medienunterst√ºtzung](/de/nodes/images)